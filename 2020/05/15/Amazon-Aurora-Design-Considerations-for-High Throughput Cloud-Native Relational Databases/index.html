<!DOCTYPE html>


<html lang="en">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases |  MYKRobin&#39;S BLOG
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  


</head>

</html>

<body>
  <div id="app">
    <main class="content on">
      <section class="outer">
  <article id="post-Amazon-Aurora-Design-Considerations-for-High Throughput Cloud-Native Relational Databases" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases
</h1>
 

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/05/15/Amazon-Aurora-Design-Considerations-for-High%20Throughput%20Cloud-Native%20Relational%20Databases/" class="article-date">
  <time datetime="2020-05-15T11:07:10.000Z" itemprop="datePublished">2020-05-15</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/paper/">paper</a> / <a class="article-category-link" href="/categories/paper/SIGMOD/">SIGMOD</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">17.2k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">60 min</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      
      

      
      <div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">属性</th>
<th style="text-align:center">内容</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">原文PDF</td>
<td style="text-align:center"><a href="https://dl.acm.org/doi/epdf/10.1145/3035918.3056101" target="_blank" rel="noopener">Aurora</a></td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">作者信息</td>
<td style="text-align:center">Amazon</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">核心内容</td>
<td style="text-align:center">分布式数据库</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">研究领域</td>
<td style="text-align:center">数据库</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">全文总览</td>
<td style="text-align:center">致力于构建一个分布式数据库</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p>研究背景</p>
</li>
<li><p>解决问题</p>
</li>
<li><p>该问题的挑战</p>
</li>
<li><p>解决方法与步骤</p>
</li>
<li><p>如何设计实验验证解决该问题的效果</p>
</li>
<li><p>相关工作，作者是如何总结、分类相关工作的。</p>
</li>
<li><p>对论文工作的批评性建议。</p>
<hr>
<h3 id="研究背景-为什么出现Aurora？"><a href="#研究背景-为什么出现Aurora？" class="headerlink" title="研究背景 为什么出现Aurora？"></a>研究背景 为什么出现Aurora？</h3></li>
</ul>
<p>&emsp; &emsp; Aurora是亚马逊云服务AWS中的关系型数据库服务，主要面向OLTP场景。 <strong>Aurora</strong>基本设计理念是在云上环境下，数据库的最大瓶颈不再是计算或者存储资源，而是网络，因此基于一套存储计算分离架构，将日志处理下推到分布式存储层，通过架构上的优化来解决网络瓶颈。 </p>
<p>&emsp; &emsp; 在云上环境下，存储计算分离作为解决系统弹性和伸缩性的方案越来越普遍。广义来说，任何数据库，底下文件系统挂一个分布式存储，即可以认为做到了存储计算分离。通过存储计算分离，可以透明添加存储节点，剔除故障节点，进行故障切换，扩展存储空间等。在这个背景下，IO不再成为数据库的瓶颈，因为IO压力可以打散在多个存储节点上，反而是<strong>网络成为瓶颈</strong>，因为数据库实例与所有存储节点的交互都需要通过网络，尤其是为了提升数据库性能，数据库实例与存储节点可能是并行交互的，这进一步加重了网络压力。</p>
<p>&emsp; &emsp; 传统数据库中的IO操作是需要同步执行的，当需要进行IO等待时，这往往会导致线程上下文切换，影响数据库性能。比如IO读操作，当需要访问一个数据页时，如果在缓冲池没有命中，则需要进行磁盘IO，那么读线程需要等待IO完成才能继续其它操作，同时这种动作可能会进一步引发刷脏页等。另外一个我们熟悉场景是事务提交操作(IO写操作)，事务提交成功返回前一定要等待事务对应日志刷盘才能返回，由于事务是串行提交，因此其它事务也必须同步等待这个事务提交。 <strong>传统数据库中的两阶段事务尤其不适合与分布式云环境，因为二阶段提交协议对系统中参与的节点和网络要求很高，自身容错能力有限，这点与大规模分布式云环境中，软件和硬件故障是常态的特征是矛盾的。</strong></p>
<p>&emsp; &emsp; 本文介绍的Aurora是一个云上环境全新的数据库服务可以很好的解决上述传统数据库遇到的问题。 <strong>它基于存储计算分离的架构，并将回放日志部分下推到分布式存储层，存储节点与数据库实例(计算节点)松耦合，并包含部分计算功能。</strong> Aurora体系下的数据库实例仍然包含了大部分核心功能，比如查询处理，事务，锁，缓存管理，访问接口和undo日志管理等；但redo日志相关的功能已经下推到存储层，包括日志处理，故障恢复，备份还原等。</p>
<h3 id><a href="#" class="headerlink" title=" "></a> </h3><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>&emsp; &emsp;Aurora诞生的原因是在弹性伸缩的云环境下，传统的高吞吐<strong>OLTP</strong>数据库既不能保证可用性，又不能保证持久性。 <strong>Aurora</strong>的关键点在于将传统数据库中的存储与计算分离，具体而言，将日志部分下推到一个独立的分布式存储服务层。由于这种分离架构下，所有IO操作都是通过网络，网络将成为最大的瓶颈，因此Aurora集中精力优化网络以便提高系统吞吐能力。Aurora依靠Quorum模型，在性能影响可控的前提下，解决云环境下的各种异常错误。在Aurora中，日志处理技术减少了I/O写放大，异步提交协议避免了同步等待，同时分离的存储服务层还避免了离线故障恢复和检查点操作。</p>
<p>&emsp; &emsp;Aurora相对于传统数据库有三大优势，首先，底层数据库存储是一个分布式存储服务，可以轻松应对故障；其次，数据库实例往底层存储层只写redo日志，因此数据库实例与存储节点之间的网络压力大大减小，这为提升数据库性能提供了保障；第三，将部分核心功能(故障恢复，备份还原)下推到存储层，这些任务可以在后台不间歇地异步执行，并且不影响前台用户任务。</p>
<hr>
<p><strong>快速修复和弥补</strong></p>
<p>数据复制的方式有多种。在传统存储系统中，数据镜像或纠删编码在整个物理存储单元进行，在多个单元组合在一个 RAID 阵列中。这种方法导致修复十分缓慢。RAID 阵列的重建性能受到阵列中少数设备的能力限制。随着存储设备变得越来越大，重建期间需要复制的数据量越多。</p>
<p>Amazon Aurora 使用完全不同的复制方法，这种方法基于分区和扩展架构。Aurora 数据库卷在逻辑上分为 10-GiB 大小的逻辑单位（保护组），每个保护组以六种方式复制到物理单位（分段）中。各个分段都分散在庞大的分布式存储队列中。如果发生某个故障，导致一个分段丢失，单个保护组的修复只需要移动大约 10GiB 的数据，几秒钟即可完成。</p>
<p>此外，需要修复多个保护组时，整个存储队列都将参与修复进程。这提供了极大的带宽，从而可以快速完成整个批次的修复操作。因此，如果某个可用区丢失并且又发生了另一个组件故障，Aurora 可能在几秒钟内失去给定保护组的写入仲裁。但自动启动的修复操作会以极快的速度恢复可写入性。换言之，<strong>Aurora 存储会快速自我恢复</strong>。</p>
<p><strong>如何实现以六种方式复制数据并保持高性能的写入？</strong> </p>
<p>传统数据库架构会将完整的页面或磁盘扇区写入存储，导致网络疲于应付，因此无法实现这一目标。与此相反，Aurora 中的实例仅将恢复日志记录写入存储。这些记录要小很多（一般为几十或几百字节），可以在不造成网络过载的情况下实现 4/6 写入仲裁。</p>
<p>根据写入仲裁的基本原理，一些片段最初可能不会始终收到所有写入。这些片段是如何处理恢复日志流中的缺口的？Aurora 存储节点会在相互之间持续“闲谈”以填补空白（并执行修复）。日志流的推进会通过日志序列号 (LSN) 管理来紧密编排。我们使用一组 LSN 标志来维护各个独立片段的状态。</p>
<p><strong>读取方面是怎么操作的？</strong> </p>
<p>仲裁读取十分昂贵，最好能够避免。客户端 Aurora 存储驱动器会跟踪哪些片段的哪些写入操作已经成功。由于它始终知道在哪里取得最新的页面副本，因此不需要为例行页面读取执行仲裁读取。此外，驱动器会跟踪读取延迟，始终尝试从过去延迟最低的存储节点读取。唯一需要仲裁读取的情形是在数据库实例重启期间的恢复。必须通过询问存储节点的方式重建初始的 LSN 标志集。</p>
<hr>
<p><strong>仲裁之美</strong></p>
<p>一切都会出现故障。系统越大，某个东西发生故障的概率越大：网络链接、SSD、整个实例、软件组件等等。即使软件组件没有漏洞，它仍然需要定期重启以进行升级。</p>
<p>传统方法是在执行故障转移前阻止 I/O 处理，以及在出现故障组件时以“降级模式”运行，这种方法在大规模环境下存在很多问题。应用程序往往也不能很好地承受 I/O“打嗝”。借助略微复杂的数学，可以证明在大型系统中，随着系统规模的增长，以降级模式运行的概率会接近 1。此外还存在一个真正潜藏的“灰色故障”问题。这是指组件并未完全失效，而是变得运行缓慢。如果系统设计没有预计延迟的问题，则这一短板可能导致整个系统的性能下降。</p>
<p>Amazon Aurora 使用了仲裁机制来解决组件故障和性能降级的问题。写入仲裁的基本原理十分简单：写入尽可能多的副本以确保仲裁读取始终可以找到最新的数据。最基本的仲裁例子是“三分之二”：</p>
<p>Vw+Vr &gt; V</p>
<p>Vw &gt; V / 2</p>
<p>V=3</p>
<p>Vw=Vr=2</p>
<p>例如，您可能需要执行 3 个物理写入，写入仲裁为 2。您无需等待所有三个写入都完成，即可宣布逻辑写操作已经成功。如果有一个写入失败或缓慢是可以接受的，因为总体操作结果和延迟不会受到此异常值的影响。这一点非常重要：即使某个东西发生故障，仍可以成功快速完成写入。</p>
<p>这种简单的 2/3 仲裁机制可让您容忍某个可用区完全丢失。当然这还不够。虽然丢失整个可用区是一种稀有事件，但不会降低其他可用区发生组件故障的可能性。<strong>对于 Aurora，我们的目标是可用区+1：我们希望能够容忍一个可用区丢失，并且同时再发生一个故障时不发生任何数据持久性损失，同时对数据可用性的影响也极低。</strong>我们使用 4/6 的仲裁机制来实现这一目标：</p>
<p>Vw+Vr &gt; V</p>
<p>Vw &gt; V / 2</p>
<p>V=6</p>
<p>Vw=4</p>
<p>Vr=3</p>
<p>对于每个逻辑日志写入，我们发出六项物理复制写入指令，在其中四项写入指令完成时视为写入操作成功。每个可用区有两个副本，如果整个可用区丢失，写入操作仍将会完成。如果一个可用区丢失，同时又发生了一个故障，您仍可达到读取仲裁要求，然后执行快速修复以快速恢复写入能力。</p>
<hr>
<p><strong>日志就是数据库</strong></p>
<p>传统关系数据库以<strong>页面</strong>的方式来组织，因此在页面修改后，必须定期刷新到磁盘。为确保在发生故障时的弹性以及维护 ACID 语法的需要，页面修改也将记录在恢复日志记录中，这些日志记录将以连续统一的方式写入磁盘。虽然这种架构提供了支持关系数据库管理系统所需的基本功能，但却存在效率低下的弊端。例如，单个逻辑数据库写入会变为多个（不超过五个）物理磁盘写入，从而引发性能问题。</p>
<p>数据库管理员会尝试通过降低页面刷新的频率来消除写入放大的问题。但这反过来又会加剧崩溃恢复持续时间的问题。刷新间隔时间越久，意味着为了重建正确的页面映像，需要从磁盘读取并应用的恢复日志记录越多。这会导致恢复速度下降。</p>
<p>在 Amazon Aurora，日志就是数据库。数据库实例将恢复日志记录写入分布式的存储层，由存储负责按需利用日志记录构建页面映像。数据库实例无需刷新页面，因为存储层始终知道页面的具体内容。这从多个方面提高了数据库的性能和可靠性。由于消除了写入放大，并且使用了扩展存储队列，写入性能得到极大的提高。</p>
<hr>
<p>关系数据库由来已久。关系数据模型可以追溯至 1970 年代 E.F.Codd 的探索。支撑当今主要关系数据库管理系统的核心技术是在 1980-1990 年代开发的。关系数据库的基本要素包括数据关系、ACID（原子性、一致性、独立性和持久性）事务、SQL 查询语言等，都经受住了时间的考验。凭借这些基本特点，关系数据库赢得了全世界用户的钟爱。它们依然是许多公司 IT 基础设施的基石之一。</p>
<p>但这并不是说系统管理员一定很喜欢处理关系数据库。数十年来，管理关系数据库一直都是一件对技能要求非常高的劳动密集型工作。它要求有专门的系统和数据库管理员全神贯注。<strong>对关系数据库进行扩展并同时保持容错能力、性能和爆炸半径大小（发生故障的影响），一直是管理员们面临的一个持久挑战。</strong></p>
<p>此外，现代互联网工作负载的要求变得越来越高，需要基础设施具备多个关键的特性：</p>
<ol>
<li>用户希望先从小规模起步，然后再大规模增长，基础设施不应限制他们的发展速度。</li>
<li>在大型系统中，故障属于常态，而非异常。发生组件故障或系统故障时，客户工作负载必须隔离。</li>
<li>爆炸半径要小。没有人希望单一的系统故障对他们的业务产生巨大影响。</li>
</ol>
<p>这些问题很难处理，需要突破传统关系数据库架构才能解决。当亚马逊公司面临 Oracle 等传统关系数据库的局限性时，我们创建了一种先进的关系数据库服务 <strong>Amazon Aurora。</strong></p>
<p>Aurora 的设计保留了关系数据库的核心事务一致性优势。它在存储层进行了创新，构建了一个面向云的数据库，可以在不牺牲性能的前提下支持现代工作负载。客户非常喜欢这一点，因为 Aurora 提供了商业级数据库的性能和可用性，但成本只有后者的十分之一。<strong>从 Amazon Aurora 最初发布以来，它一直是 AWS 历史上增长最为快速的服务</strong>。</p>
<hr>
<p><strong>作为新一代数据库 Amazon Aurora究竟有哪些优势？</strong></p>
<p>作为亚马逊为云打造的一款能兼容MySQL的新一代企业级数据库，Amazon Aurora基于云设计了一套全新架构，使数据库的性能大大优化，其速度最高可以达到标准MySQL数据库的五倍、标准 PostgreSQL数据库的三倍。而且，Amazon Aurora不仅拥有高端商业数据库的性能和可用性，还拥有非常灵活的横向及纵向扩展能力，具有开源数据库的简单性和成本效益。</p>
<p>这样的数据库一定很贵吧？事实并非如此，其成本仅有商业数据库的1/10而已。具体来说，Amazon Aurora在如下几个层面有着较为突出的优势：</p>
<p><strong>1、在兼容性上</strong>，Amazon Aurora能很好地兼容MySQL和PostgreSQL的关系数据库。其中，当前使用MySQL5.6和MySQL5.7的用户，无需修改应用的代码、应用程序、驱动程序和连接工具，就可以像访问原数据库一样访问Amazon Aurora。有版本差异的数据库，也只需要进行数据库升级或者进行少量代码、应用的修改就可以将系统迁移到Amazon Aurora。</p>
<p><strong>2、在高性能和可扩展性上</strong>，Amazon Aurora作为一个集群数据库，可以包含一个主节点和多个只读节点，当用户访问量增加时，企业可以根据需求动态增加只读节点，从而扩展数据库的读的能力，它最多可以跨三个可用区，添加15个低延时（一般延时为毫秒级）的只读副本，这意味着你的数据库可以是一个16个节点组成的集群，大大提高了数据库的处理能力。</p>
<p><strong>3、在数据的持久性上</strong>，Amazon Aurora数据库的存储是以SSD硬盘为基础、能被 Aurora 数据库内多个节点共享的虚拟集群卷，这个集群卷可以跨3个可用区。这意味着企业将数据插入到Aurora数据库时，数据会跨3个可用区自动复制数据的6个副本，从而在出现硬盘故障或者数据中心灾难时可以自动恢复，从而保障数据的高可用。</p>
<p><strong>4、在数据的访问上</strong>，Amazon Aurora数据库的访问一般通过集群终端节点和只读节点，这与普通的MySQL类似，其中集群终端节点会访问数据库主节点进行读写操作，只读节点通过负载均衡的方式访问多个只读节点，以减轻每个节点的压力，如果主节点出现故障，只读节点会自动提升为主节点。而Amazon Aurora的多个节点可以位于云上不同的可用区，这进一步提升了数据库的访问能力。</p>
<p><strong>5、在数据库架构上</strong>，传统数据库是将数据库实例和存储集成在一起，很难横向扩展，性能也不易改善；而Amazon Aurora是基于云设计的新一代架构，使用的是计算和存储分离的方案，在高可用、存储、日志、连接等多方面有优化。这样一来，计算层和存储层之间传输的是日志而不是更改的数据，因此计算和存储层之间的网络流量大大降低。</p>
<p><strong>6、在数据库维护上</strong>，Amazon Aurora是一种完全托管的数据库，管理员无须管理数据库的操作系统及操作系统上软件的安装等，亚马逊自动定期将数据库数据及日志备份到Amazon S3存储，从而使数据库管理员在需要时快速恢复数据，管理员可以很容易将数据库恢复到5分钟前的任何状态。Amazon Aurora还提供了回退功能，如果用户出现了误操作，管理员可以将数据库回退到3天内的任何一个一致状态。这些无疑大大减轻了管理员的工作压力。</p>
<p>从上述几大特点我们可以清晰看出，Amazon Aurora的优势着重体现了对云计算时代企业应用的关注。如果用一个词来形容，Amazon Aurora更像是一个“云原生”的数据库，而不是传统数据库的优化和演进。这估计也是Amazon Aurora为何成为AWS公司历史上发展最快的服务的核心所在。</p>
<hr>
<p>Aurora的计算节点和存储节点分离，分别位于不同的VPC（Virtual Private Cloud）中。这是Aurora架构最亮眼之处。</p>
<p>如图1，用户的应用通过Customer VPC接入，然后可以读写位于不同AZ（Availability Zone）的数据库。而不同的AZ分布于全球不同的Region中（如图2，截止到2017年初，AWS全球有16个区域即Region，有42个可用区即AZ，每个Region至少有2个AZ。而每个AZ由两到多个数据中心组成，数据中心不跨AZ，每个AZ内部的数据延迟低于0.25ms。AWS文档称，AZ之间的延迟低于2ms，通常小于1ms。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQBYw6uscwalUib0vmic1vbK59Cy4dQDY9Hmdv5HdUX6crzxDrM3eEYNug/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图1  Aurora整体架构"></p>
<p>图1 Aurora整体架构</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQ76t9qibbWgXfwV54lvn962v3fzSvA5fw6Et2IhfHXWibkHexE7m4JefA/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图2  Aurora的Region分布"></p>
<p>图2 Aurora的Region分布</p>
<p>数据库的部署，是一主多从的集群架构，图1的Primary RW DB是写数据的节点，只能有一个（这点说明Aurora还是传统的数据库架构，不是真正的对等分布式架构，这点也是一些批评者认为Aurora缺乏真正创新之处的缘由）。而Secondary RO DB是只读的从节点，由零到多个备节点组成，最多可以有15个。主从节点可以位于不同的AZ（最多位于3个VPC，需要3个AZ），但需要位于同一个Region内，节点通过RDS（Relational Database Service）来交互。</p>
<p>RDS是由位于每个节点上称为（HM(Host Manager）的agent来提供主从集群的状态监控，以应对主节点fail over的问题以便进行HA调度，以及某个从节点fail over需要被替换等问题。这样的监控服务，称为Control Plane。</p>
<p>数据库的计算服务和存储分离，数据缓冲区和持久化的“数据”（对于Aurora实则是日志和由日志转化来的以page为单位的数据，而不是直接由数据缓冲区刷出的page存储的数据）位于Storage VPC中，这样和计算节点在物理层面隔离。一个主从实例，其物理存储需要位于同一个Region中，这样的存储称为EC2 VMs集群，其是由一个个使用了SSD的Storage Node组成。</p>
<p>核心技术与架构</p>
<h3 id="Aurora提倡“the-log-is-the-database”，这是其设计的核心。围绕这个观点，传统数据库的组件架构发生了一些变化。"><a href="#Aurora提倡“the-log-is-the-database”，这是其设计的核心。围绕这个观点，传统数据库的组件架构发生了一些变化。" class="headerlink" title="Aurora提倡“the log is the database”，这是其设计的核心。围绕这个观点，传统数据库的组件架构发生了一些变化。"></a>Aurora提倡“the log is the database”，这是其设计的核心。围绕这个观点，传统数据库的组件架构发生了一些变化。</h3><p>对于Aurora，每一个存储节点，如图3，由两部分构成。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQR3pbTUEqd3ySL4xoq1Y3yQwYs4Xb1qZZuOf9ch0a7GvgH3FHluPw2Q/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图3  存储结构"></p>
<p>图3 存储结构</p>
<ul>
<li><strong>第一部分：Caching</strong></li>
</ul>
<p>这是一个重要的关键点，可惜论文没有描述其细节。</p>
<p>如同传统数据库架构的数据缓冲区，向事务层提供数据。传统数据库架构的数据缓冲区，向上起着消耗存储I/O的I加载数据到内存供计算层读写数据的作用；向下起着消耗I/O的O写出脏数据到存储层以实现数据持久存储的作用。对于一个写密集的OLTP系统，大量随机写花费了很多时间，系统的性能因此经常表现为存储层的I/O瓶颈。 尽管checkpoint技术缓解了每个写操作刷出脏数据的冲动，尽管SSD的使用缓解了存储层的瓶颈，但是，毕竟存储层的I与O的时间消耗还是巨大的，尤其是对于随机写密集的OLTP系统。</p>
<p>Aurora的设计消除了脏数据刷出的过程，数据缓冲区的作用只是加载数据供上层使用，而脏数据不必从数据缓冲区刷出到物理存储上，这对于随机写密集的OLTP系统而言，是一个福音，性能的瓶颈点被去掉了一个（如图3，在“Caching”和“Logging+Storage”之间，竖线的箭头应该是指向“Caching”的，以表示数据只是加载到Caching中，不存在脏数据的刷出操作）。</p>
<p>但是，观察图3，“Caching”是位于了存储层内还是计算层内？论文没有明示。</p>
<p>从图3观察，似乎“Caching”是存储层和计算层所共用的一个组件，那么就可能存在这样的一个两层设计：位于存储层和计算层各有一部分“Caching”，这两部分“Caching”组合成为一个逻辑上的“Caching”，而逻辑意义上的“Caching”在AWS似乎认为其更像是属于计算层的。如下文引自论文原文：</p>
<blockquote>
<p><em>Although each instance still includes most of the components of a traditional kernel (query processor, transactions, locking, buffer cache, access methods and undo management) several functions (redo logging, durable storage, crash recovery,and backup/restore) are off-loaded to the storage service.</em></p>
</blockquote>
<p>位于存储层内的“Caching”，更像是一个分布式的共享文件系统，为了提高性能也许是一个分布式内存型的共享文件系统，为主从架构的数据库提供高速读服务，此点妙处，论文没有点出，这里权做推测。存储层如果能为所有的主备节点提供一致的缓冲数据，则有更为积极的意义，可以对比参考的如Oracle的RAC。</p>
<p>而位于计算层内的“Caching”，是单个数据库实例读数据的场所，独立使用。</p>
<p>Aurora提供了一个“自动恢复”缓存预热的功能，其官方宣称如下：</p>
<blockquote>
<p>“自动恢复”缓存预热</p>
<p>当数据库在关闭后启动或在发生故障后重启时，Aurora将对缓冲池缓存进行“预热”。即，Aurora会用内存页面缓存中存储的已知常用查询页面预加载缓冲池。这样，缓冲池便无需从正常的数据库使用“预热”，从而提高性能。</p>
<p>Aurora页面缓存将通过数据库中的单独过程进行管理，这将允许页面缓存独立于数据库进行“自动恢复”。在出现极少发生的数据库故障时，页面缓存将保留在内存中，这将确保在数据库重新启动时，使用最新状态预热缓冲池。</p>
<p>源自：<a href="http://docs.amazonaws.cn/AmazonRDS/latest/UserGuide/Aurora.Overview.html" target="_blank" rel="noopener">http://docs.amazonaws.cn/AmazonRDS/latest/UserGuide/Aurora.Overview.html</a></p>
</blockquote>
<p>“在出现极少发生的数据库故障时，页面缓存将保留在内存中”，这句话很重要，一是其表明数据不用很耗时地重新加载了；二是数据库实例崩溃前的数据内存状态被保留着；三是数据库崩溃重启不必再执行“故障恢复”的过程即使用REDO日志重新回放以保障数据的一致性了（事务的ACID中的C特性）。</p>
<p>那么，页面缓冲是一直保留在哪个节点的内存中？是存储节点还是计算节点？如果是位于计算节点，那么备机节点发生数据库故障时，这样的机制不会对备机节点起到保护作用。如果是位于存储节点，则存储作为一个服务，服务了一主多备的多个节点，则能更好地发挥“自动恢复”缓冲预热的功效（存储节点的Caching一直存在，向上层计算节点的Caching提供数据批量加载服务，但也许不是这样，而是提供一个接口，能够向计算层的Caching提供高速读数据的服务，论文没有更多的重要细节披露，权做推测）。由此看来，“Caching”层的两层设计，当是有价值的（价值点是“自动恢复”缓冲预热，由存储层提供此项服务），与预写日志功能从事务层剥离是关联的设计。</p>
<p>这就又回到前面引用的论文中的那段英文，其表明 ：Aurora的设计，把REDO日志、持久化存储、系统故障恢复、物理备份与物理恢复这些功能模块，归属到了存储层。由此就引出了Aurora的另外一个重要话题——存储层的设计（如下的第二部分和下一节内容）。</p>
<p>对于计算层的“Caching”，其实现将被大为简化。脏数据不再被写出，脏页面不再需要复杂的淘汰策略进行管理，消除了后台的写任务线程，同时也消除了checkpoint线程的工作，数据缓冲区的管理大为简化，既降低了系统的复杂度又减少了时间的消耗，还避免了因执行后台写等任务带来的性能抖动，解耦带来的功效确实宜人。Aurora额外需要做的一项新工作是：only pages with a long chain of modifications need to be rematerialized。而计算层的“Caching”变成单向的读入，此时需要解决的仅仅是什么样的数据可以（从存储层的Caching）被读入的问题，而论文原文描述：</p>
<blockquote>
<p><em>The guarantee is implemented by evicting a page from the cache only if its “page LSN” (identifying the log record associated with the latest change to the page) is greater than or equal to the VDL.</em></p>
</blockquote>
<p>VDL是存储层的最小一致点（参见下文“Aurora的事务处理-持久性”这一节的内容），标识了可用日志的最低范围，比VDL还老的数据页不再可用，所以显然如上的论文原文是错误的。如果有比当前数据页还新的数据页被从日志中恢复，则其LSN一定更大，所以页面换入的条件是：存储层Caching中存在页面的LSN值更大；页面被换出的条件是：Caching中页面的LSN小于等于VDL。而且，这一定是发生在备机需要更新其计算层Caching的时刻，而不是主机需要更新其计算层Caching的时刻。存在此种情况，其原因已经很明显，主机修改数据，形成脏页，这样的脏页（数据的后像）才能作为REDO日志的一部分被主机刷出。而主机不会刷出脏页，所以被修改后的数据页应该一直在内存中，而被修改过的数据页如果反复被修改，则意味着主机Caching中的相应脏页数据一定是最新的，没有必要从存储层的Caching中读入“绕道恢复后的数据页”。如果以上猜想不成立，除非Aurora生成REDO日志时，存于REDO日志中的数据页部分采取先复制然后其上的数据项被修改这样的方式。可是多做一次复制，又有何必要呢？</p>
<p>另外，如果“Caching”确实存在两层（另外一个证据，参见图4 ），而如“存储层的工作”一节所述，存储层也在处理日志，并依据日志生成页数据，则存储节点也存在处理数据的能力，就类似于Oracle的ExaData。这样可能导致两层的“Caching”还是存在差别的。存储层的“Caching”能够帮助做谓词下推的工作，然后把较少的数据传回计算层的“Caching”，由此实现类似Oracle ExaData的智能扫描（Smart Scan）功能。是否如此，或者Aorora的体系结构和功能模块在未来继续演变的时候，是否会在存储层内的“Caching”做足文章，可以拭目以待。不过，目前制约存储层内“Caching”起更大作用的因素，主要在于分布式事务机制的选取和InnoDB自身的事务实现机制。详细讨论参见下文“事务与数据分布”一节的内容。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQmvickhlLlP1O5SVxibM31LyicHricVzqWc1ib942vRzs8fjpicptBUKbn2ibQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图4  存储层的“Shared storage column”与计算层的“Caching”构成的两层数据缓冲结构"></p>
<p>图4 存储层的“Shared storage column”与计算层的“Caching”构成的两层数据缓冲结构</p>
<ul>
<li><strong>第二部分：Logging+Storage</strong></li>
</ul>
<p>这部分是日志和持久化存储。日志与传统数据库对于预写日志（WAL）的利用方式与MySQL不同，这点是Aurora实现计算与存储分离的核心（下一节详述存储层实现细节）。</p>
<p>如图5所示，对于日志数据，从Primary RW DB写出到一个存储节点，每个AZ至少有2份数据，写出的日志数据会自动复制到3个AZ中的6个存储节点，当其中的多数节点回应写日志成功，则向上层返回写成功的ACK。这表明写日志信息采用了多数派协议（Quorum）。</p>
<p>MySQL的事务模型符合SS2PL协议 ，当日志成功写出，就可以在内存中标识事务提交成功 ，而写日志信息是一个批量的、有序的I/O操作，再加上Aurora去除了大量的缓冲区脏数据的随机写操作，因此Aurora的整体性能得到大幅提升。</p>
<p>借用官方论文的一组对比数据，可以感性认识存储和计算分离所带来的巨大好处，如图6所示，MySQL的每个事务的I/O花费是Aurora的7.79倍，而事务处理量Aurora是MySQL的35倍，相差明显。</p>
<p>对于主备系统之间，如图5所示，主备之间有事务日志（LOG）和元数据（FRM FILES）的传递。也就是说备机的数据是源自主机的。如图5所示的主备之间的紫色箭头，表示主机向备机传输的是更新了的元数据，绿色箭头表示日志作为数据流被发送给了备机（这个复制应该是异步的，相关内容请参考下文“存储层的工作”一节）。所以备机的数据更新，应该是应用了主机传输来的事务日志所致。这是论文中表述的内容，原文如下：</p>
<blockquote>
<p><em>In this model, the primary only writes log records to the storage service and streams those log records as well as metadata updates to the replica instances.</em></p>
</blockquote>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQySDzhhYl4OsTnia4Lrxum0ORmKJ7Ee0sFyhXUSLZibT7xetJswIJDQwg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图5  主从复制日志存储图 "></p>
<p>图5 主从复制日志存储图</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQWqSlxaAN1UF1vuiaLr9aYfS5TGZ7DTNwLz6HLdrK4iaeDBFwqmZQCBMw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图6  Aurora与MySQL主从复制架构性能数据对比"></p>
<p>图6 Aurora与MySQL主从复制架构性能数据对比</p>
<p>但是，日志的应用功能是被放到了存储层实现的，如原文描述：</p>
<blockquote>
<p><em>Instead, the log applicator is pushed to the storage tier where it can be used to generate database pages in background or on demand.</em></p>
</blockquote>
<p>而官方的网站用图7描述了备机的数据，是从存储节点读入的。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/ZIKRVBYbQdeKTmXgbGZ8y0EKn0joXkuQE81icbVib3boOhjKzkEMtePq9HqeKNp0bwW1Pyg3FDVamHWN0yd4jf9g/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图7  Aurora主备机数据流"></p>
<p>图7 Aurora主备机数据流</p>
<p>鉴于以上几点，备机数据获取和更新的这个细节，算是个谜。</p>
<p>“Caching”如果确实分为两层，在存储层提供从日志中恢复成为数据页的形式而被缓冲，则主备系统之间应该没有必要再传输日志数据，对于备机而言，直接从统一的存储层的“Caching”中获取数据即可。</p>
<p>与此相关的一个问题是：为什么备机节点可以多达15个呢？难道仅仅是应对读负载吗？ 或者，作为故障转移的目标，需要这么多备机做备选吗？这又是一个谜。</p>
<hr>
<p>Amazon在SIGMOD 2017发表了论文《Amazon Aurora: DesignConsiderations for High Throughput Cloud-Native Relational Databases》，第一次公开介绍了Aurora的设计理念和内部实现，下文是我对论文的解读，如有理解不准确的地方，欢迎大家批评指正。</p>
<h3 id="gt-gt-摘要"><a href="#gt-gt-摘要" class="headerlink" title="&gt;&gt;摘要"></a><strong>&gt;&gt;</strong>摘要</h3><p>Aurora是亚马逊云服务AWS中的关系型数据库服务，主要面向OLTP场景。本文会详细介绍Aurora的架构以及设计背后的理念。 <strong>Aurora</strong> <strong>基本设计理念是在云上环境下，数据库的最大瓶颈不再是计算或者存储资源，而是网络，因此基于一套存储计算分离架构，将日志处理下推到分布式存储层，通过架构上的优化来解决网络瓶颈。</strong> 下文首先会介绍Aurora如何做到不仅减少了网络资源消耗，同时还能快速故障恢复且不丢失数据，接着会介绍Aurora如何做到异步模式下分布式存储节点的一致性，最后会介绍Aurora在生产环境使用的经验。</p>
<h3 id="gt-gt-1-概述"><a href="#gt-gt-1-概述" class="headerlink" title="&gt;&gt;1. 概述"></a><strong>&gt;&gt;</strong>1. 概述</h3><p>在云上环境下，存储计算分离作为解决系统弹性和伸缩性的方案越来越普遍。广义来说，任何数据库，底下文件系统挂一个分布式存储，即可以认为做到了存储计算分离。通过存储计算分离，可以透明添加存储节点，剔除故障节点，进行故障切换，扩展存储空间等。在这个背景下，IO不再成为数据库的瓶颈，因为IO压力可以打散在多个存储节点上，反而是网络成为瓶颈，因为数据库实例与所有存储节点的交互都需要通过网络，尤其是为了提升数据库性能，数据库实例与存储节点可能是并行交互的，这进一步加重了网络压力。传统数据库中的IO操作是需要同步执行的，当需要进行IO等待时，这往往会导致线程上下文切换，影响数据库性能。比如IO读操作，当需要访问一个数据页时，如果在缓冲池没有命中，则需要进行磁盘IO，那么读线程需要等待IO完成才能继续其它操作，同时这种动作可能会进一步引发刷脏页等。另外一个我们熟悉场景是事务提交操作(IO写操作)，事务提交成功返回前一定要等待事务对应日志刷盘才能返回，由于事务是串行提交，因此其它事务也必须同步等待这个事务提交。 <strong>传统数据库中的两阶段事务尤其不适合与分布式云环境，因为二阶段提交协议对系统中参与的节点和网络要求很高，自身容错能力有限，这点与大规模分布式云环境中，软件和硬件故障是常态的特征是矛盾的。</strong></p>
<p>本文介绍的Aurora是一个云上环境全新的数据库服务可以很好的解决上述传统数据库遇到的问题。 <strong>它基于存储计算分离的架构，并将回放日志部分下推到分布式存储层，存储节点与数据库实例(计算节点)松耦合，并包含部分计算功能。</strong> Aurora体系下的数据库实例仍然包含了大部分核心功能，比如查询处理，事务，锁，缓存管理，访问接口和undo日志管理等；但redo日志相关的功能已经下推到存储层，包括日志处理，故障恢复，备份还原等。Aurora相对于传统数据库有三大优势，首先，底层数据库存储是一个分布式存储服务，可以轻松应对故障；其次，数据库实例往底层存储层只写redo日志，因此数据库实例与存储节点之间的网络压力大大减小，这为提升数据库性能提供了保障；第三，将部分核心功能(故障恢复，备份还原)下推到存储层，这些任务可以在后台不间歇地异步执行，并且不影响前台用户任务。下文会详细介绍Aurora如何实现这些功能，主要包括三大块：<br>1.如何基于Quorum模型保证底层存储的一致性<br>2.如何将redo日志相关的功能下推到存储层<br>3.如何消除同步点，分布式存储下如何做检查点和故障恢复</p>
<h3 id="gt-gt-2-可扩展高可用存储"><a href="#gt-gt-2-可扩展高可用存储" class="headerlink" title="&gt;&gt; 2. 可扩展高可用存储"></a><strong>&gt;&gt;</strong> 2. 可扩展高可用存储</h3><h4 id="2-1复制和容错处理"><a href="#2-1复制和容错处理" class="headerlink" title="2.1复制和容错处理"></a>2.1复制和容错处理</h4><p>Aurora存储层的复制基于Quorum协议，假设复制拓扑中有V个节点，每个节点有一个投票权，读 或 写 必须拿到Vr 或 Vw个投票才能返回。为了满足一致性，需要满足两个条件，首先Vr + Vw &gt; V，这个保证了每次读都能读到拥有最新数据的节点；第二，Vw &gt; V/2，每次写都要保证能获取到上次写的最新数据，避免写冲突。比如V=3，那么为了满足上述两个条件，Vr=2，Vw=2。为了保证各种异常情况下的系统高可用，Aurora的数据库实例部署在3个不同AZ(AvailablityZone)，每个AZ包含了2个副本，总共6个副本，每个AZ相当于一个机房，是一个独立的容错单元，包含独立的电源系统，网络，软件部署等。结合Quorum模型以及前面提到的两条规则， <strong>V=6</strong> <strong>，Vw=4，Vr=3，Aurora可以容忍任何一个AZ出现故障，不会影响写服务；任何一个AZ出现故障，以及另外一个AZ中的一个节点出现故障，不会影响读服务且不会丢失数据。</strong></p>
<h4 id="2-2分片管理"><a href="#2-2分片管理" class="headerlink" title="2.2分片管理"></a>2.2分片管理</h4><p>通过Quorum协议，Aurora可以保证只要AZ级别的故障(火灾，洪水，网络故障)和节点故障(磁盘故障，掉电，机器损坏)不同时发生，就不会破坏协议本身，数据库可用性和正确性就能得到保证。那么，如果想要数据库“永久可用”，问题变成如何降低两类故障同时发生的概率。由于特定故障发生的频率(MTTF, Mean Time to Fail)是一定的，为了减少故障同时发生的概率，可以想办法提高故障的修复时间(MTTR,Mean Time To Repair)。Aurora将存储进行分片管理，每个分片10G，6个10G副本构成一个PGs(Protection Groups)。Aurora存储由若干个PGs构成，这些PGs实际上是EC2(AmazonElastic Compute Cloud)服务器+本地SSD磁盘组成的存储节点构成，目前Aurora最多支持64T的存储空间。分片后，每个分片作为一个故障单位，在10Gbps网络下，一个10G的分片可以在10s内恢复，因此当前仅当10s内同时出现大于2个的分片同时故障，才会影响数据库服务的可用性，实际上这种情况基本不会出现。 <strong>通过分片管理，巧妙提高了数据库服务的可用性。</strong></p>
<h4 id="2-3轻量级运维"><a href="#2-3轻量级运维" class="headerlink" title="2.3轻量级运维"></a>2.3轻量级运维</h4><p>基于分片管理，系统可以灵活应对故障和运维。比如，某个存储节点的磁盘IO压力比较大，可以人为将这个节点剔除，并快速新加一个节点到系统。另外，在进行软件升级时，同样可以临时将存储节点剔除，待升级完毕后再将节点加入到系统。所有这些故障和运维管理都是分片粒度滚动进行的，对于用户完全透明。</p>
<h3 id="gt-gt-3-存储计算分离"><a href="#gt-gt-3-存储计算分离" class="headerlink" title="&gt;&gt; 3. 存储计算分离"></a><strong>&gt;&gt;</strong> 3. 存储计算分离</h3><h4 id="3-1传统数据库写放大问题"><a href="#3-1传统数据库写放大问题" class="headerlink" title="3.1传统数据库写放大问题"></a>3.1传统数据库写放大问题</h4><p>我们看看在传统数据库中写的流程。以单机MySQL为例，执行写操作会导致日志落盘，同时后台线程也会异步将脏页刷盘，另外为了避免页断裂，进行刷脏页的过程还需要将数据页写入double-write区域。如果考虑生产环境中的主备复制，如图2所示，AZ1和AZ2分别部署一个MySQL实例做同步镜像复制，底层存储采用Elastic Block Store(EBS)，并且每个EBS还有自己的一份镜像，另外部署Simple Storage Service(S3)进行redo日志和binlog日志归档，以支持基于时间点的恢复。从流程上来看，每个步骤都需要传递5种类型的数据，包括redo，binlog，data-page，double-write和frm元数据。由于是基于镜像的同步复制，这里我理解是Distributed Replicated Block Device(DRBD)，因此<strong>图中的1，3，5步骤是顺序的，这种模型响应时间非常糟糕，因为要进行4次网络IO，且其中3次是同步串行的。从存储角度来看，数据在EBS上存了4份，需要4份都写成功才能返回。 </strong> 所以在这种架构下，无论是IO量还是串行化模型都会导致性能非常糟糕。**</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Hk1ceA7cQUC57AiaXe7gG8ypOJUwQRLcawSp2WuRLicYC5gdt9FkoLcZETHF8L34qwiaoUWzq9c5zPCxX4CbtpuvQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<h4 id="3-2日志处理下放到存储层"><a href="#3-2日志处理下放到存储层" class="headerlink" title="3.2日志处理下放到存储层"></a>3.2日志处理下放到存储层</h4><p>传统数据库中，修改一个数据页，会同步产生对应的redo日志，基于数据页的前镜像回放redo日志可以得到数据页的后镜像。事务提交时，需要事务对应的redo日志都写盘成功后才能返回。在Aurora中，所有的写类型只有一种，那就是redo日志，任何时候都不会写数据页。存储节点接收redo日志，基于旧版本数据页回放日志，可以得到新版本的数据页。为了避免每次都从头开始回放数据页变更产生的redo日志，存储节点会定期物化数据页版本。如图3所示， <strong>Aurora**</strong>由跨AZ的一个主实例和多个副本实例组成，主实例与副本实例或者存储节点间只传递redo日志和元信息。主实例并发向6个存储节点和副本实例发送日志，当4/6的存储节点应答后，则认为日志已经持久化，对于副本实例，则不依赖其应答时间点。** 从sysbench测试(100G规模，只写场景，压测30分钟)的数据来看，Aurora是基于镜像MySQL吞吐能力的35倍，每个事务的日志量比基于镜像MySQL日志量要少7.7倍。再来看看故障恢复速度，传统数据库宕机重启后，恢复从最近的一个检查点开始，读取检查点后的所有redo日志进行回放，确保已经提交的事务对应的数据页得到更新。在Aurora中，redo日志相关的功能下推到存储层，回放日志的工作可以一直在后台做。任何一次读磁盘IO操作，如果数据页不是最新版本，都会触发存储节点回放日志，得到新版本的数据页。因此类似传统数据库的故障恢复操作实质在后台一直不断地进行，而真正进行故障恢复时，需要做的事情很少，所以故障恢复的速度非常快。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Hk1ceA7cQUC57AiaXe7gG8ypOJUwQRLcajz35MHWadPYRw2ibNLib3pyWZInAiaqLnPL4NDBnGDEhBlpRibn7ibz4Few/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>3.3存储服务设计关键点</p>
<p>Aurora存储服务设计的一个关键原则是减少前台用户写的响应时间，因此将尽可能多的操作移到后台异步执行，并且存储节点会根据前台的请求压力，自适应分配资源做不同的工作。比如，当前台请求很繁忙时，存储节点会减缓对旧版本数据页的回收。传统数据库中，后台线程需要不断地推进检查点，避免故障恢复时间消耗的时间过长，但会影响前台用户请求处理能力； <strong>对于Aurora而言，分离的存储服务层使得后台线程推进检查点动作完全不影响数据库实例，并且是推进地越快，越有利于前台的磁盘IO读操作(减少了回放日志过程)。</strong> Aurora写基于Quorum模型，存储分片后，按片达成多数派即可返回，由于分布足够离散，少数的磁盘IO压力大也不会影响到整体的写性能。如图4所示，图中详细介绍了主要的写流程，1).存储节点接收数据库实例的日志，并追加到内存队列；2).将日志在本地持久化成功后，给实例应答；3).按分片归类日志，并确认丢失了哪些日志；4).与其它存储节点交互，填充丢失的日志；5).回放日志生成新的数据页；6).周期性地备份数据页和日志到S3系统；7).周期性地回收过期的数据页版本；8).周期性地对数据页进行CRC校验。上述所有写相关的操作，只有第1)和第2)步是串行同步的，会直接影响前台请求的响应时间，其它操作都是异步的。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Hk1ceA7cQUC57AiaXe7gG8ypOJUwQRLcazlHwicbhShia2mJ8ibwVfzT3T1nL8x6Nq4zYUEqfVLQzhp3rkmDnS3iaGw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p><strong>&gt;&gt;</strong> 4.一致性原理</p>
<p>这节主要介绍Aurora如何在不利用2PC协议的情况下，如何通过在读副本，存储节点间传递redo日志保证数据一致性。首先，我们会介绍如何做到在故障恢复时不需要回放redo日志；其次，我们会介绍常见的操作，比如读，写和事务提交操作，然后会介绍Aurora如何保证从数据库副本实例读取的数据处于一致的状态；最后会详细介绍故障恢复的流程。</p>
<h4 id="4-1日志处理"><a href="#4-1日志处理" class="headerlink" title="4.1日志处理"></a>4.1日志处理</h4><p>目前市面上几乎所有的数据库都采用WAL(Write Ahead Logging)日志模型，任何数据页的变更，都需要先写修改数据页对应的redo日志，Aurora基于MySQL改造当然也不例外。在实现中，每条redo日志拥有一个全局唯一的Log Sequence Number(LSN)。为了保证多节点数据的一致性，我们并没有采用2PC协议，因为2PC对错误的容忍度太低，取而代之的是，我们基于Quorum协议来保证存储节点的一致性。由于在生产环境中，各个节点可能会缺少部分日志，各个存储节点利用gossip协议补全本地的redo日志。 <strong>在正常情况下，数据库实例处于一致的状态，进行磁盘IO读时，只需要访问redo日志全的存储节点即可；但在故障恢复过程中，需要基于Quorum协议进行读操作，重建数据库运行时的一致状态。</strong> 数据库实例中活跃着很多事务，事务的开始顺序与提交顺序也不尽相同。当数据库异常宕机重启时，数据库实例需要确定每个事务最终要提交还是回滚。这里介绍下Aurora中存储服务层redo日志相关几个关键的概念。Volumn Complete LSN(VCL)，表示存储服务拥有VCL之前的所有完整的日志。在故障恢复时，所有LSN大于VCL的日志都要被截断。ConsistencyPoint LSNs(CPLs)，对于MySQL(InnoDB)而言，如下图所示， <strong>每个事务在物理上由多个mini-transaction组成，而每个mini-transaction是最小原子操作单位，比如B树分裂可能涉及到多个数据页的修改，那么这些页修改对应的对应一组日志就是原子的，重做日志时，也需要以mini-transaction为单位。</strong> CPL表示一组日志中最后的一条日志的LSN，一个事务由多个CPL组成，所以称之为CPLs。Volumn Durable LSN(VDL)表示已持久化的最大LSN，是所有CPLs中最大的LSN，VDL&lt;=VCL，为了保证不破坏mini-transaction原子性，所有大于VDL的日志，都需要被截断。比如，VCL是1007，LSN为900，1000，1100是CPLs，那么我们需要截断1000以前的日志。 <strong>VDL</strong> <strong>表示了数据库处于一致状态的最新位点，在故障恢复时，数据库实例以PG为单位确认VDL，截断所有大于VDL的日志。</strong> </p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Hk1ceA7cQUC57AiaXe7gG8ypOJUwQRLcaibreBUTa5X0FbpkrfVN0DibVEFcI5cA6OK9OSgt2o1VnoYlaDUqkdkKg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>4.2基本操作</p>
<h5 id="1-Writes"><a href="#1-Writes" class="headerlink" title="1).Writes"></a>1).Writes</h5><p><strong>在Aurora中，数据库实例向存储节点传递redo日志，达成多数派后将事务标记为提交状态，然后推进VDL，使数据库进入一个新的一致状态。</strong> 在任何时刻，数据库中都会并发运行着成千上万个事务，每个事务的每条redo日志都会分配一个唯一的LSN，这个LSN一定大于当前最新的VDL，为了避免前台事务并发执行太快，而存储服务的VDL推进不及时，我们定义了LSN Allocation Limit(LAL)，目前定义的是10,000,000，这个值表示新分配LSN与VDL的差值的最大阀值，设置这个值的目的是避免存储服务成为瓶颈，进而影响后续的写操作。由于底层存储按segment分片，每个分片管理一部分页面，当一个事务涉及的修改跨多个分片时，事务对应的日志被打散，每个分片只能看到这个事务的部分日志。 <strong>为了确保各个分片日志的完整性，每条日志都记录前一条日志的链接，通过前向链接确保分片拥有了完整的日志。Segment Complete LSN(SCL)表示分片拥有完整日志的位点，存储节点相互间通过gossip协议来弥补本地日志空洞，推进SCL</strong> 。</p>
<h5 id="2-Commits"><a href="#2-Commits" class="headerlink" title="2).Commits"></a>2).Commits</h5><p>在Aurora中，事务提交是完全异步的。每个事务由若干个日志组成，并包含有一个唯一的“commit LSN”，工作线程处理事务提交请求时，将事务相关的日志提交到持久化队列并将事务挂起，并继续处理其它数据库请求。 <strong>当VDL的位点大于事务的commit LSN时，表示这个事务redo日志都已经持久化，可以向客户端回包，通知事务已经成功执行。在Aurora中，有一个独立的线程处理事务成功执行的回包工作，因此，从整个提交流程来看，所有工作线程不会因为事务提交等待日志推进而堵塞</strong> ，他们会继续处理新的请求，通过这种异步提交方式，大大提高了系统的吞吐。这种异步化提交思想目前比较普遍，AliSQL也采用类似的方式。</p>
<h5 id="3-Reads"><a href="#3-Reads" class="headerlink" title="3).Reads"></a>3).Reads</h5><p>在Aurora中，与大多数数据库一样，数据页的请求一般都从缓冲池中获得，当缓冲池中对应的数据页不存在时，才会从磁盘中获取。如果缓冲池满了，根据特定的淘汰算法(比如LRU)，系统会选择将一个数据页淘汰置换出去，如果被置换的数据页被修改过，则首先需要将这个数据页刷盘，确保下次访问这个页时，能读到最新的数据。但是Aurora不一样，淘汰出去的数据页并不会刷盘写出，而是直接丢弃。 <strong>这就要求Aurora缓冲池中的数据页一定有最新数据，被淘汰的数据页的page-LSN需要小于或等于VDL。(注意，这里论文中描述有问题，page-LSN&lt;=VDL才能被淘汰，而不是大于等于)</strong> 这个约束保证了两点：1.这个数据页所有的修改都已经在日志中持久化，2.当缓存不命中时，通过数据页和VDL总能得到最新的数据页版本。</p>
<p><strong>在正常情况下，进行读操作时并不需要达成Quorum。当数据库实例需要读磁盘IO时，将当前最新的VDL作为一致性位点read-point，并选择一个拥有所有VDL位点的日志的节点作为请求节点，这样只需要访问这一个节点即可得到数据页的最新版本。</strong> 从实现上来看，因为所有数据页通过分片管理，数据库实例记录了存储节点管理的分片以及SCL信息，因此进行IO操作时，通过元信息可以知道具体哪个存储节点有需要访问的数据页，并且SCL&gt;read-point。数据库实例接收客户端的请求，以PG为单位计算Minimum Read Point LSN，在有读副本实例的情况下，每个实例都都可以作类似的计算得到位点，实例之间通过gossip协议得到全局的per-Group MRPL，称之为PGMRPL。PGMRPL是全局read-point的低水位，每个存储节点根据PGMRPL，不断推进数据页版本，并回收不再使用的日志。</p>
<h5 id="4-Replicas"><a href="#4-Replicas" class="headerlink" title="4).Replicas"></a>4).Replicas</h5><p><strong>在Aurora中，写副本实例和至多15个读副本实例共享一套分布式存储服务，因此增加读副本实例并不会消耗更多的磁盘IO写资源和磁盘空间。这也是共享存储的优势，零存储成本增加新的读副本。</strong>读副本和写副本实例间通过日志同步。写副本实例往存储节点发送日志的同时向读副本发送日志，读副本按日志顺序回放， <strong>如果回放日志时，对应数据页不在缓冲池中，则直接丢弃。可以丢弃的原因在于，存储节点拥有所有日志，当下次需要访问这个数据页时，存储节点根据read-point，可以构造出特定的数据页版本</strong> 需要说明的是，写副本实例向读副本发送日志是异步的，写副本执行提交操作并不受读副本的影响。副本回放日志时需要遵守两个基本原则，1).回放日志的LSN需要小于或等于VDL，2).回放日志时需要以MTR为单位，确保副本能看到一致性视图。在实际场景下，读副本与写副本的延时不超过20ms。</p>
<h4 id="4-3-故障恢复"><a href="#4-3-故障恢复" class="headerlink" title="4.3 故障恢复"></a>4.3 故障恢复</h4><p>大多数数据库基于经典的ARIES协议处理故障恢复，通过WAL机制确保故障时已经提交的事务持久化，并回滚未提交的事务。这类系统通常会周期性地做检查点，并将检查点信息计入日志。故障时，数据页中同时可能包含了提交和未提交的数据，因此，在故障恢复时，系统首先需要从上一个检查点开始回放日志，将数据页恢复到故障时的状态，然后根据undo日志回滚未交事务。从故障恢复的过程来看， <strong>故障恢复是一个比较耗时的操作，并且与检查点操作频率强相关。通过提高检查点频率，可以减少故障恢复时间，但是这直接会影响系统处理前台请求吞吐，所以需要在检查点频率和故障恢复时间做一个权衡，而在Aurora中不需要做这种权衡。</strong></p>
<p>传统数据库中，故障恢复过程通过回放日志推进数据库状态，重做日志时整个数据库处于离线状态。<strong>Aurora**</strong>也采用类似的方法，区别在于将回放日志逻辑下推到存储节点，并且在数据库在线提供服务时在后台常态运行。** 因此，当出现故障重启时，存储服务能快速恢复，即使在10wTPS的压力下，也能在10s以内恢复。数据库实例宕机重启后，需要故障恢复来获得运行时的一致状态，实例与Read Quorum个存储节点通信，这样确保能读到最新的数据，并重新计算新的VDL，超过VDL部分的日志都可以被截断丢弃。在Aurora中，对于新分配的LSN范围做了限制，LSN与VDL差值的范围不能超过10,000,000，这个主要是为了避免数据库实例上堆积过多的未提交事务，因为数据库回放完redo日志后还需要做undo recovery，将未提交的事务进行回滚。在Aurora中，收集完所有活跃事务后即可提供服务，整个undo recovery过程可以在数据库online后再进行。</p>
<h3 id="gt-gt-5-云上Aurora体系"><a href="#gt-gt-5-云上Aurora体系" class="headerlink" title="&gt;&gt; 5. 云上Aurora体系"></a><strong>&gt;&gt;</strong> 5. 云上Aurora体系</h3><p>在社区InnoDB中，一个写操作会修改缓冲池中数据页内容，并将对应的redo日志按顺序写入WAL。事务提交时，WAL协议约定对应事务的日志需要持久化后才能返回。实际上，为了防止页断裂，缓冲池中修改的数据页也会写入double-write区域。数据页的写操作在后台进行，一般是在页面置换或是做检查点过程中发生。InnoDB除了IO子系统，还包括事务子系统，锁管理系统，B+Trees实现以及MTR等。MTR约定了最小事务，MTR中的日志必需以原子的方式执行(比如B+Tree分裂或合并相关的数据页)。<br>数据库引擎基于社区版InnoDB引擎改进，将磁盘IO读写分离到存储服务层。redo日志按照PG划分，每个MTR的最后一条日志是一致性位点。与社区的MySQL版本一样，支持标准的隔离级别和快照读。Aurora数副本实例不断从写副本实例获取事务开始和提交信息，并利用该信息提供快照读功能。 <strong>数据库实例与存储服务层相互独立，存储服务层向上为数据库实例提供统一的数据视图，数据库实例从存储服务层获取数据与从本地读取数据一样。</strong> 下图展示了云上Aurora的部署架构，Aurora利用AmazonRelational Database Service (RDS)来管理元数据。RDS在每个实例部署一个agent，称之为Host Manager(HM)。HM监控集群的健康状况并确定是否需要做异常切换，或者是一个实例是否需要重建。每个集群由一个写副本，0个或多个读副本组成。所有实例在都一个物理Region(比如美国东部，美国西部)，一般是跨AZ部署，并且分布式存储服务层也在同一个Region。为了保证安全，我们在数据库层，应用层和存储层做了隔离。实际上，数据库实例通过3类Amazon Virtual Private Cloud (VPC)网络可以相互通信。通过应用层VPC，应用程序可以访问数据库；通过RDS VPC，数据库可以和管控节点交互；通过存储层VPC，数据库可以和存储服务节点交互。<br>存储服务实际上是由一组EC2集群构成，这个集群横跨至少3个AZ，对多个用户提供存储，读写IO，备份恢复等服务。存储节点管理本地SSD并与数据库实例和其它存储节点交互，备份/还原服务不断备份新数据到S3，在必要时从S3还原数据。存储服务的管控系统借助Amazon DynamoDB服务作为持久化存储，存储内容包括配置，元数据信息，备份到S3数据的信息等。为了保证存储服务高可用，整个系统需要在异常影响到用户之前，主动快速发现问题。系统中的所有关键操作都被监控，一旦性能或者可用性方面出现问题，则立即会产生报警。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Hk1ceA7cQUC57AiaXe7gG8ypOJUwQRLcaHBLwXicQn0roVaf3V7vJN5MRibqEUW6gm33ycUrstmgyRnpZExQ6rqew/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<h3 id="gt-gt-8-总结"><a href="#gt-gt-8-总结" class="headerlink" title="&gt;&gt; 8. 总结"></a><strong>&gt;&gt;</strong> 8. 总结</h3><p>Aurora诞生的原因是在弹性伸缩的云环境下，传统的高吞吐OLTP数据库既不能保证可用性，又不能保证持久性。 <strong>Aurora</strong>的关键点在于将传统数据库中的存储与计算分离，具体而言，将日志部分下推到一个独立的分布式存储服务层。由于这种分离架构下，所有IO操作都是通过网络，网络将成为最大的瓶颈，因此Aurora集中精力优化网络以便提高系统吞吐能力。Aurora依靠Quorum模型，在性能影响可控的前提下，解决云环境下的各种异常错误。在Aurora中，日志处理技术减少了I/O写放大，异步提交协议避免了同步等待，同时分离的存储服务层还避免了离线故障恢复和检查点操作。** Aurora的存储计算分离方案使得系统整体架构非常简单，而且方便未来的演进。</p>
<h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p><strong>1.</strong>  <strong>一般了解到的Quorum算法只需要满足Vr + Vw &gt; N即可，为啥Aurora还需要满足第2个条件？ </strong><br>从文中了解到Aurora中Quorum算法需要满足两个条件：<br>a).Vr + Vw &gt; N，NWR算法，确保读和写有交集，能读到最新写入的数据。<br>b).Vw &gt; N/2，避免更新冲突。<br>Quorum算法的基本含义是确保读副本数与写副本数有交集，能读到最新写入的数据。Aurora加入第二条约束主要是为了保证每次写入集合都与上次写入的节点集合有交集，确保能读到最近一次的更新，这样日志能够以自增的方式追加，确保不会丢失更新。从另外一个角度来说，写副本数设置为Vw &gt; N/2也间接在读写性能做了均衡。假设N=5，W=1，R=5，满足第一个条件，每次写入一个节点即可返回成功，那么读取时则需要读取5个副本，才能拿到最新的数据。</p>
<p><strong>2.</strong>  <strong>每个segment分片是10G，如果事务跨多个segment分片如何处理？</strong><br>每个事务实际是有若干个MTR(mini-transaction)构成，MTR是InnoDB中修改物理块的最小原子操作单位，MTR的redo日志作为一个整体写入到全局redo日志区。在Aurora中存储按照segment分片管理，发送日志时，也按分片归类后发送，那么就存在一种情况，一个MTR跨多个segement分片，MTR日志被打散的情况。本质上来说，这种情况也不存在问题，因为 <strong>事务提交时，如果事务跨多个segment分片，则会需要多个PG都满足Quorum协议才返回，进而推进VDL。所以如果VDL超过了事务的commit-LSN，则表示事务涉及到的日志已经全部持久化，不会存在部分segment日志丢失的问题，所以能够保证事务持久性。</strong></p>
<p><strong>3.Aurora 读副本如何实现MVCC？</strong><br>在Aurora中，写副本实例往存储节点发送日志的同时会传递日志到读副本实例，读副本以MTR为单位回放日志；与此同时，写副本还会将事务开始和提交的信息传递给读副本，这样在读副本实例上也能构造出活跃的事务视图。在读副本上执行读操作时，会依据活跃事务视图作为记录可见习判断依据，进而能读到合适版本的数据。当然，Aurora读副本与写副本之间是异步复制，至多有20ms的延迟，因此在Aurora读副本上可能不能读到最新的提交数据。</p>
<p><strong>4.</strong> <strong>为什么Aurora有成本优势？</strong><br>Aurora中，多个数据库实例(写副本+多个读副本)共享一个分布式存储层。对于数据库引擎而言，整个存储服务一个大的资源池，用户根据需求申请存储空间，最小粒度是10G，相对于单个实例的本地存储，存储空间利用率大大提升，而且非常方便扩展。另一方面，所有数据库引擎公用一份存储，零存储成本增加数据库引擎，能大幅降低成本。</p>
<p><strong>5.Aurora</strong> <strong>的优势和缺陷？</strong><br>优势：<br>1). 存储节点，计算节点弹性伸缩，按需配比<br>2). 一份存储，对接多个计算节点，多租户存储服务，成本低。<br>3). 只传递日志，巧妙的解决写放大问题。<br>4). 实例快速故障恢复<br>5). 架构简单，通过多副本能快速扩展读能力，单个写副本则巧妙地避免了分布式事务等复杂实现。</p>
<p>缺陷：<br>1). 适合于读多写少的应用，写水平扩展依赖于中间件方案。<br>2). SQL层与社区版MySQL一样，复杂查询能力(比如OLAP场景)较弱。<br>3). 单个写副本，无分区多点写能力(用户维度+时间维度)<br>4). 总容量有上限，64TB</p>
<p><strong>6.Aurora</strong> <strong>与Spanner的异同点？</strong> </p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/Hk1ceA7cQUC57AiaXe7gG8ypOJUwQRLcaEDB71jjReHyqwN7DH5k2QzWakEQ4ziap8O2KNnf7NjtwFNmIKvk3scw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>从对比来看，Aurora与Spanner走的两条不同的路线，Aurora以市场用户为导向，所以选择全面兼容MySQL/PostgreSQL,用户可以无缝迁移到Aurora。另外就是，Aurora基于MySQL修改，并通过共享存储方案避免了二阶段提交，分布式事务等复杂的实现，因此开发周期相对较短，也更容易出成果。反观Spanner，是一个重新设计的数据库，无论是基于Paxos的强同步还是分布式事务的支持，以及利用TrueTime机制实现全局一致性读等都是比较复杂的实现，功能非常强大，但是在SQL兼容性这块确做地不够好，这也可以解释为什么Aurora广泛用于云上业务，而Spanner则更多地是在Google内部使用，相信即使Spanner现在开放了云版本，其SQL兼容性也是一个很大的挑战。当然Aurora本身的数据库引擎就是MySQL，其对于复杂查询的短板还需要持续改进和优化。</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
        <div class="declare">
          <ul class="post-copyright">
            <li>
              <i class="ri-copyright-line"></i>
              <strong>Copyright： </strong>
              Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
            </li>
          </ul>
        </div>
        
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        share
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://ykma.gitee.io/2020/05/15/Amazon-Aurora-Design-Considerations-for-High%20Throughput%20Cloud-Native%20Relational%20Databases/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/" rel="tag">数据库</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/" rel="tag">论文解读</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/05/18/SP-15-Last-Level-Cache-Side-Channel-Attacks-are-Practical/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            SP&#39;15 Last-Level Cache Side-Channel Attacks are Practical
          
        </div>
      </a>
    
    
      <a href="/2020/05/03/Lares-An-Architecture-for-Secure-Active-Monitoring-Using-Virtualization/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Lares:An Architecture for Secure Active Monitoring Using Virtualization</div>
      </a>
    
  </nav>


  

  

  
  
  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020
        <i class="ri-heart-fill heart_icon"></i> Mykrobin
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
    <ul>
			    <!-- 网站运行时间的设置 -->
			<span id="timeDate">载入天数...</span>
			<span id="times">载入时分秒...</span>
			<script>
			    var now = new Date();
			    function createtime() {
			        var grt= new Date("04/18/2020 10:00:00");//此处修改你的建站时间或者网站上线时间
			        now.setTime(now.getTime()+250);
			       // years = (now - grt) / 1000 / 60 /60 / 24 / 365;       
			       // ynum = Math.floor(years);
			        days = (now - grt ) / 1000 / 60 / 60 / 24; 
			        dnum = Math.floor(days);
			        restdnum = Math.floor(days);
			        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); 
			        hnum = Math.floor(hours);
			        if(String(hnum).length ==1 )
			        	{hnum = "0" + hnum;} 
			        minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
			        mnum = Math.floor(minutes); 
			        if(String(mnum).length ==1 )
			        	{mnum = "0" + mnum;}
			        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
			        snum = Math.round(seconds); 
			        if(String(snum).length ==1 )
			        	{snum = "0" + snum;}
			        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+ " 天 ";
			        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
			    }
			setInterval("createtime()",250);
			</script>
    </ul>    
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="MYKRobin&#39;S BLOG"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/picture/">画廊</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/">论文</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/homework/">课程</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/essay/">杂谈</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/game-for-peace/">娱乐</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i></p>
  <div class="reward-box">
    
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script>
  try {
    var typed = new Typed("#subtitle", {
      strings: ['面朝大海，春暖花开', '愿你一生努力，一生被爱', ''],
      startDelay: 1,
      typeSpeed: 200,
      loop: true,
      backSpeed: 200,
      showCursor: true
    });
  } catch (err) {
  }

</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>



<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>



<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>


<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>



    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script>
        
    




<script src="/js/busuanzi-2.3.pure.min.js"></script>



<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>

  
<script src="/js/clickLove.js"></script>



<!-- 复制 -->

  
<link rel="stylesheet" href="/css/clipboard.css">

  <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>




    
  </div>
</body>

</html>